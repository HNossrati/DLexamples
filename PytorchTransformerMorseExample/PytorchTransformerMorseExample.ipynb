{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# A Comprehensive Guide to PyTorch's nn.Transformer() Module: Morse Code Translation Example\n\n## Overview and Learning Objectives\n\nWelcome to an in-depth exploration of the Transformer architecture using PyTorch, with a fascinating practical application: Morse Code Translation! This tutorial will guide you through building a neural machine translation model using the revolutionary Transformer architecture introduced in the seminal paper [\"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762) by Vaswani et al.\n \n ### Key Learning Objectives:\n \n - Understand the core components of the Transformer architecture.\n - Implement a Transformer model for sequence-to-sequence translation.\n - Learn how to preprocess and prepare data for sequence translation.\n - Gain practical experience with PyTorch's `nn.Transformer()` module.\n \n ### Background: Transformers and Their Revolution\n \n The Transformer architecture, introduced in 2017, fundamentally changed how we approach sequence-to-sequence tasks. Unlike previous recurrent neural network (RNN) architectures, Transformers rely entirely on attention mechanisms, enabling:\n - More parallel computation.\n - Effective handling of long-range dependencies.\n \n ### Why Morse Code Translation?\n Morse code provides an intuitive, constrained domain for demonstrating sequence translation:\n - Binary nature (dots and dashes) simplifies the task.\n - Fixed mapping between characters.\n - Historical significance as a communication method.\n - A practical example for showcasing sequence-to-sequence learning principles.\n %% [markdown]\n ## Prerequisites and Dependencies\n \n Before diving into the code, ensure the following libraries are installed:\n - `PyTorch`: Core deep learning framework.\n - `NumPy`: For numerical computations.\n - `Matplotlib`: For plotting metrics.\n - `torchview` (optional): For model visualization.\n - `graphviz` (optional): Used by `torchview` for graphical representations.\n \n **Note**: This implementation is designed for educational purposes and demonstrates core Transformer principles. Real-world applications require advanced techniques and larger datasets.\n\n**Note**: You may freely use or reproduce our work but please cite it.techniques and larger datasets.","metadata":{"_uuid":"a36682ea-f41a-4a30-960d-4db3e0c2b37c","_cell_guid":"d866546f-69bb-4855-8cbd-a00e2f8ad47f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"\n## Imports and Environment Setup\n\nIn the first code cell, we'll import the necessary libraries and set up a consistent random seed for reproducibility. Seeding helps ensure that our random operations produce the same results across different runs.\n\n### Key Considerations:\n- `torch.manual_seed()` ensures reproducible random number generation\n- `torch.backends.cudnn.deterministic` enforces deterministic operations\n- We use NumPy's random seed to maintain consistency across libraries","metadata":{"_uuid":"8401e3e0-4864-4f41-b8f8-181314ea7a8d","_cell_guid":"eb7b9cec-9c4f-4ed8-bf47-27c97603c091","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\ntry:\n    from torchview import draw_graph\n    import graphviz\nexcept ImportError:\n    import pip\n    pip.main(['install', 'torchview', 'graphviz'])\n    from torchview import draw_graph\n    import graphviz\nfrom torchinfo import summary","metadata":{"_uuid":"d51c1b72-6d4e-44cb-8f3e-7ecba5f76d6f","_cell_guid":"edb448da-fc22-42f1-bfe8-b3dc0091e5f9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-16T14:54:24.740743Z","iopub.execute_input":"2024-12-16T14:54:24.741078Z","iopub.status.idle":"2024-12-16T14:54:36.105381Z","shell.execute_reply.started":"2024-12-16T14:54:24.741038Z","shell.execute_reply":"2024-12-16T14:54:36.104493Z"}},"outputs":[{"name":"stderr","text":"WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\nPlease see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\nTo avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Collecting torchview\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Collecting torchview\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  Downloading torchview-0.2.6-py3-none-any.whl.metadata (12 kB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Downloading torchview-0.2.6-py3-none-any.whl.metadata (12 kB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Requirement already satisfied: graphviz in /opt/conda/lib/python3.10/site-packages (0.20.3)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: graphviz in /opt/conda/lib/python3.10/site-packages (0.20.3)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading torchview-0.2.6-py3-none-any.whl (25 kB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Downloading torchview-0.2.6-py3-none-any.whl (25 kB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Installing collected packages: torchview\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Installing collected packages: torchview\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Successfully installed torchview-0.2.6\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Successfully installed torchview-0.2.6\n</pre>\n"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"\ndef seed_everything(seed=512):\n    \"\"\"\n    Seed everything.\n    \"\"\"\n    # random.seed(seed)\n    # os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n\n\nseed_everything()","metadata":{"_uuid":"43dfad9a-32bf-4c4b-aacb-f5d540e12615","_cell_guid":"7cbfa012-3642-401d-b0b4-f6c22848bc70","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n## Morse Code Dictionary and Preprocessing\n\nNext, we define our Morse code dictionary, which maps characters to their binary representations. This is a critical step in preparing our translation task.\n\n### Morse Code Encoding:\n- Each character is represented by a unique binary sequence\n- '0' typically represents a short signal (dot)\n- '1' typically represents a long signal (dash)","metadata":{"_uuid":"63796108-4b5e-42a8-952c-aa95128f9441","_cell_guid":"1eaee255-9dd4-43c1-98f3-8e0c04f5ada4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"\n# Define Morse Code Dictionary\nMORSE_CODE_DICT = {\n    'A': '01', 'B': '1000', 'C': '1010', 'D': '100', 'E': '0',\n    'F': '0010', 'G': '110', 'H': '0000', 'I': '00', 'J': '0111',\n    'K': '101', 'L': '0100', 'M': '11', 'N': '10', 'O': '111',\n    'P': '0110', 'Q': '1101', 'R': '010', 'S': '000', 'T': '1',\n    'U': '001', 'V': '0001', 'W': '011', 'X': '1001', 'Y': '1011',\n    'Z': '1100',\n}","metadata":{"_uuid":"6f439bbe-60f7-4714-9d0f-73a051f075b5","_cell_guid":"572dd20b-55d8-4104-891b-d03a93a83d78","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Special Tokens:\nWe introduce special tokens to help our model understand sequence boundaries:\n- `SOS_TOKEN`: Start of Sequence\n- `EOS_TOKEN`: End of Sequence\n- `PADDING_TOKEN`: Used to make sequences uniform length\n- `SPACE_CHAR`: Used to separate characters from each other in morse code","metadata":{"_uuid":"952b7b79-0dbd-4aa7-95ce-7a84bcb060a6","_cell_guid":"7091d695-6f8c-44cd-b54e-d0c3519cf773","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Special tokens\nSOS_TOKEN = '<SOS>'\nEOS_TOKEN = '<EOS>'\nPADDING_TOKEN = '<PAD>'\nSPACE_CHAR = ' '","metadata":{"_uuid":"36a5cdf6-6da2-447b-9682-eec648aad900","_cell_guid":"ea4a2d7b-fef8-4090-821f-36472a7c3c8a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n\n## Dataset Generation Function\n\nThe `generate_dataset()` function is crucial in creating synthetic training data for our Morse code translation task. Let's break down its key components:\n\n### Data Generation Strategy:\n- Randomly select characters from our Morse code dictionary\n- Convert characters to their Morse code representations\n- Add space tokens between Morse code characters for clarity\n- Control sequence length through `max_len` parameter\n\n### Key Design Considerations:\n- Generates diverse training samples\n- Allows control over dataset complexity\n- Simulates real-world variability in input sequences","metadata":{"_uuid":"439bb617-3e2c-4865-ad19-66b14d4bc248","_cell_guid":"1a029eea-1c88-4884-ac67-19f55d09abf4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"\n# Reverse dictionary for decoding\nREVERSE_MORSE_CODE_DICT = {v: k for k, v in MORSE_CODE_DICT.items()}\n\n# Prepare the dataset\ndef generate_dataset(num_samples=1000, max_len=10):\n    \"\"\"Generates random Morse code sequences with corresponding text translations.\"\"\"\n    dataset = []\n    for _ in range(num_samples):\n        seq_length = random.randint(1, max_len)\n        text_seq = [random.choice(list(MORSE_CODE_DICT.keys())) for _ in range(seq_length)]\n        morse_seq = [MORSE_CODE_DICT[char] for char in text_seq]\n        morse_seq = f'{SPACE_CHAR}'.join(morse_seq)  # Add a space token between characters\n        dataset.append((morse_seq, text_seq))\n    return dataset","metadata":{"_uuid":"d8c07ed8-c866-4311-8dc6-6fdf5d42c658","_cell_guid":"caef4cf1-8a91-47d6-abc3-4e0302993ad2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Preparation and Vocabulary Creation\n\nThe `prepare_data()` function transforms our raw Morse code and text sequences into tensor representations suitable for neural network training.\n\n### Preprocessing Steps:\n1. Convert sequences to vocabulary indices\n2. Add special tokens (SOS, EOS)\n3. Pad sequences to uniform length\n4. Convert to PyTorch tensors","metadata":{"_uuid":"8bd11401-41b1-43ca-82b8-3fa4120dbdd8","_cell_guid":"d9c92f48-9978-4dcf-82be-be7b34890e33","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"\n# Convert Morse and text to indexed tensors\ndef prepare_data(dataset, morse_vocab, text_vocab, max_morse_len, max_text_len):\n    \"\"\"Converts sequences into padded tensors with indexes.\"\"\"\n    morse_sequences, text_sequences = [], []\n    for morse_seq, text_seq in dataset:\n        morse_tensor = [morse_vocab[SOS_TOKEN]] + [morse_vocab[ch] for ch in morse_seq] + [morse_vocab[EOS_TOKEN]]\n        text_tensor = [text_vocab[SOS_TOKEN]] + [text_vocab[ch] for ch in text_seq] + [text_vocab[EOS_TOKEN]]\n        \n        morse_tensor += [morse_vocab[PADDING_TOKEN]] * (max_morse_len - len(morse_tensor))\n        text_tensor += [text_vocab[PADDING_TOKEN]] * (max_text_len - len(text_tensor))\n        \n        morse_sequences.append(morse_tensor)\n        text_sequences.append(text_tensor)\n    return torch.tensor(morse_sequences, dtype=torch.long), torch.tensor(text_sequences, dtype=torch.long)","metadata":{"_uuid":"48964180-070f-47f6-b16d-20788c70a9c8","_cell_guid":"47166b5d-39b7-4378-af31-becabcd566d5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Vocabulary Mapping:\nWe create two key vocabularies:\n- `morse_vocab`: Maps Morse code tokens to unique indices\n- `text_vocab`: Maps text characters to unique indices","metadata":{"_uuid":"7204abdd-4874-45d7-83c2-bf4b0aa0ce70","_cell_guid":"391d3d7e-409b-4dab-afc3-d30f6e182398","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Define vocabulary\nmorse_vocab = {ch: idx for idx, ch in enumerate([PADDING_TOKEN, SOS_TOKEN, EOS_TOKEN, SPACE_CHAR, '0', '1'])}\ntext_vocab = {ch: idx for idx, ch in enumerate([PADDING_TOKEN, SOS_TOKEN, EOS_TOKEN] + list(MORSE_CODE_DICT.keys()))}\nreverse_text_vocab = {idx: ch for ch, idx in text_vocab.items()}","metadata":{"_uuid":"cc942324-d7d6-436f-af2e-253acaaac749","_cell_guid":"4e412a44-d2a9-42db-9a7a-3d6232b6bb5e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Splitting Data:\n- Divide dataset into training and validation sets\n- Use 80% of data for training, 20% for validation\n- Ensures model can generalize beyond training data","metadata":{"_uuid":"b1cb5c7d-8546-4039-a1f1-e044e70780b5","_cell_guid":"8a1c8b76-647a-4c9c-b811-eaa59c831df9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"\n# Dataset parameters\nNUM_SAMPLES = 80000\nTRAIN_RATIO = 0.8\nNUM_TRAIN = int(NUM_SAMPLES * TRAIN_RATIO)\nMAX_MORSE_LEN = 100\nMAX_TEXT_LEN = 12\n\ndata = generate_dataset(NUM_SAMPLES)\nmorse_tensor, text_tensor = prepare_data(data, morse_vocab, text_vocab, MAX_MORSE_LEN, MAX_TEXT_LEN)\n\ntrain_morse, val_morse = morse_tensor[:NUM_TRAIN], morse_tensor[NUM_TRAIN:]\ntrain_text, val_text = text_tensor[:NUM_TRAIN], text_tensor[NUM_TRAIN:]","metadata":{"_uuid":"e3eb9e60-7856-44dc-a765-80eb89fa994e","_cell_guid":"40309e15-3588-41f2-bcba-1f4980075d22","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n## Positional Encoding: Adding Sequence Context\n\nThe `PositionalEncoding` class is critical in helping Transformers understand the order of tokens. Unlike RNNs, Transformers process entire sequences simultaneously, so we need to inject position information.\n\n### Positional Encoding Techniques:\n- Use sine and cosine functions to create unique position embeddings\n- Embeddings have different frequencies, allowing unique representations\n- Add these encodings to input embeddings\n\n### Why Positional Encoding?\n- Provides sequence order information\n- Allows model to distinguish between tokens based on their position\n- Enables parallel processing while maintaining sequence context","metadata":{"_uuid":"44000e12-1b07-4a8b-bd42-b436ff14fa70","_cell_guid":"55dd8bf5-7a01-4048-801e-e69fede682e1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Positional Encoding\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=500):\n        super(PositionalEncoding, self).__init__()\n        self.encoding = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n        self.encoding[:, 0::2] = torch.sin(position * div_term)\n        self.encoding[:, 1::2] = torch.cos(position * div_term)\n        self.encoding = self.encoding.unsqueeze(0)\n\n    def forward(self, x):\n        return x + self.encoding[:, :x.size(1), :].to(x.device)","metadata":{"_uuid":"3f4a4a3c-7a54-49a9-a10c-c92e4edb2ba9","_cell_guid":"65631570-1de0-4652-bcac-413515fc3950","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n## Transformer Model Architecture\n\nThe `TransformerModel` class represents our complete neural translation model. It encapsulates the entire Transformer architecture, including embedding layers, positional encoding, and the core Transformer module.\n\n### Model Components:\n1. **Input Embedding**: Convert Morse code tokens to dense vector representations\n2. **Target Embedding**: Convert text tokens to dense vector representations\n3. **Positional Encoding**: Add sequence position information\n4. **Transformer Module**: Core sequence-to-sequence translation mechanism\n5. **Output Layer**: Project transformer outputs to vocabulary space\n\n### Key Design Principles:\n- Separate embeddings for input (Morse) and target (text) sequences\n- Scale embeddings by `sqrt(d_model)` to control variance\n- Use multiple attention heads for capturing different representation aspects\n\n### Attention Mechanism\n- The `generate_square_subsequent_mask()` method creates a causal mask\n- Prevents decoder from attending to future tokens during training\n- Crucial for autoregressive generation","metadata":{"_uuid":"9f8e0dfa-460b-4274-8ac5-3960579de712","_cell_guid":"48def78a-b2df-4dc6-ae58-65f585c80c27","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Transformer Model\nclass TransformerModel(nn.Module):\n    def __init__(self, input_vocab_size, target_vocab_size, d_model=512, nhead=8, num_layers=3, dim_feedforward=2048):\n        super(TransformerModel, self).__init__()\n        self.d_model = d_model\n\n        # Embedding layers\n        self.input_embedding = nn.Embedding(input_vocab_size, d_model)\n        self.target_embedding = nn.Embedding(target_vocab_size, d_model)\n        self.positional_encoding = PositionalEncoding(d_model)\n\n        # Transformer\n        self.transformer = nn.Transformer(\n            d_model=d_model, nhead=nhead, num_encoder_layers=num_layers, \n            num_decoder_layers=num_layers, dim_feedforward=dim_feedforward\n        )\n\n        # Output layer\n        self.fc_out = nn.Linear(d_model, target_vocab_size)\n\n    def generate_square_subsequent_mask(self, sz):\n        mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n        return mask\n\n    def forward(self, src, tgt):\n        src_mask = None\n        tgt_mask = self.generate_square_subsequent_mask(tgt.size(1)).to(src.device)\n        \n        src_emb = self.positional_encoding(self.input_embedding(src) * np.sqrt(self.d_model))\n        tgt_emb = self.positional_encoding(self.target_embedding(tgt) * np.sqrt(self.d_model))\n\n        output = self.transformer(src_emb.transpose(0, 1), tgt_emb.transpose(0, 1),\n                                  src_mask=src_mask, tgt_mask=tgt_mask)\n        output = self.fc_out(output.transpose(0, 1))\n        return output","metadata":{"_uuid":"201a0152-bb7e-42d7-9e82-d381cea85a1e","_cell_guid":"585f5bd7-91cf-4d0f-96f0-4d30e4cfe95c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SaveBestModel:\n    \"\"\"\n    Class to save the best model while training. If the current epoch's\n    validation loss is less than the previous least less, then save the\n    model state.\n    \"\"\"\n    def __init__(\n        self, best_valid_loss=float('inf')\n    ):\n        self.best_valid_loss = best_valid_loss\n\n    def __call__(\n        self, current_valid_loss,\n        epoch, model, optimizer\n    ):\n        if current_valid_loss < self.best_valid_loss:\n            self.best_valid_loss = current_valid_loss\n            print(f\"\\nBest loss: {self.best_valid_loss}\")\n            print(f\"\\nSaving best model for epoch: {epoch}\\n\")\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n\n                }, 'bstmdl.pth')","metadata":{"_uuid":"71416f76-8fb3-404e-bccd-c79c11d427a4","_cell_guid":"d07a90a9-e267-4843-8b0a-a619463b2e9c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n\n## Training and Evaluation Infrastructure\n\nWe've designed comprehensive training and evaluation functions:\n\n### `train()` Function:\n- Manages model training loop\n- Tracks training and validation losses\n- Implements best model checkpoint saving\n- Provides real-time training progress monitoring","metadata":{"_uuid":"5b6eb48a-f786-4509-baf6-1621c57ccafb","_cell_guid":"9f6adf6f-7042-4138-9378-0ed53c02b3b7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def train(model, train_data, val_data, optimizer, criterion, num_epochs=10, batch_size=32):\n\n    savebest = SaveBestModel()\n    \n    train_morse, train_text = train_data\n    val_morse, val_text = val_data\n    \n    # Create DataLoader for training data\n    train_dataset = torch.utils.data.TensorDataset(train_morse, train_text)\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    \n    Losses_Train = torch.empty(num_epochs)\n    Losses_Valid = torch.empty(num_epochs)\n    Accs_Train = torch.empty(num_epochs)\n    Accs_Valid = torch.empty(num_epochs)\n    \n    model.train()\n    for epoch in range(num_epochs):\n        total_loss = 0\n        correct = 0\n        total = 0\n        \n        for batch_idx, (src, tgt) in enumerate(train_loader):\n            src, tgt = src.to(device), tgt.to(device)\n            tgt_input = tgt[:, :-1]  # Input to the decoder (everything except the last token)\n            tgt_output = tgt[:, 1:]  # Ground truth (everything except the first token)\n\n            optimizer.zero_grad()\n            output = model(src, tgt_input)\n\n            # Calculate loss\n            loss = criterion(output.reshape(-1, output.size(-1)), tgt_output.reshape(-1))\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n            # Calculate accuracy\n            pred = output.argmax(2)  # Predicted tokens\n            correct += (pred == tgt_output).sum().item()\n            total += tgt_output.numel()\n\n            if (batch_idx + 1) % 100 == 0:\n                print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], \"\n                      f\"Loss: {loss.item():.4f}\")\n\n        val_loss, val_acc = evaluate(model, val_data, criterion, batch_size)\n\n        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader):.4f}, \"\n              f\"Accuracy: {correct / total:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}\")\n        savebest(total_loss / len(train_loader), epoch + 1, model, optimizer)\n        \n        Losses_Train[epoch] = total_loss / len(train_loader)\n        Losses_Valid[epoch] = val_loss\n        Accs_Train[epoch] = correct / total  \n        Accs_Valid[epoch] = val_acc\n    \n    history = {\"Loss_Train\"     : Losses_Train, \n               \"Loss_Valid\"     : Losses_Valid, \n               \"Accuracy_Train\" : Accs_Train,  \n               \"Accuracy_Valid\" : Accs_Valid,}\n    return history","metadata":{"_uuid":"4bb13a2c-a9a3-4839-9435-e828ae2eeabe","_cell_guid":"34ac2302-d5a4-4ecb-b3ad-402a19b667cc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### `evaluate()` Function:\n- Performs model evaluation on validation dataset\n- Computes loss and accuracy metrics\n- Uses no-gradient context for efficiency","metadata":{"_uuid":"dcc8b950-7b91-4b39-91cc-916819e588ff","_cell_guid":"f10a864a-60e1-458e-a06b-45e450be37f2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"\ndef evaluate(model, val_data, criterion, batch_size=32):\n    model.eval()\n    val_morse, val_text = val_data\n    \n    # Create DataLoader for validation data\n    val_dataset = torch.utils.data.TensorDataset(val_morse, val_text)\n    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)\n    \n    total_loss = 0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for src, tgt in val_loader:\n            src, tgt = src.to(device), tgt.to(device)\n            tgt_input = tgt[:, :-1]  # Input to the decoder\n            tgt_output = tgt[:, 1:]  # Ground truth\n\n            output = model(src, tgt_input)\n\n            # Calculate loss\n            loss = criterion(output.reshape(-1, output.size(-1)), tgt_output.reshape(-1))\n            total_loss += loss.item()\n\n            # Calculate accuracy\n            pred = output.argmax(2)\n            correct += (pred == tgt_output).sum().item()\n            total += tgt_output.numel()\n\n    return total_loss / len(val_loader), correct / total","metadata":{"_uuid":"32287595-48ca-4b91-8299-5c0793131561","_cell_guid":"1667fa1c-192e-41b3-a819-608fa6fc8946","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Optimization Strategies:\n- Adam optimizer for adaptive learning rates\n- Cross-entropy loss with padding token ignore\n- Batch processing for computational efficiency","metadata":{"_uuid":"e0f59fc3-c51d-462e-a8d1-6f8cf409b4f2","_cell_guid":"138128f7-6936-459a-aabc-821a4508f169","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n\n# Instantiate the model\nmodel = TransformerModel(\n    input_vocab_size=len(morse_vocab),\n    target_vocab_size=len(text_vocab),\n    d_model=256,  # Reduced for faster training\n    nhead=16,\n    num_layers=3,\n    dim_feedforward=256  # Reduced for faster training\n).to(device)\n\n# Define optimizer and loss function\noptimizer = optim.Adam(model.parameters(), lr=0.0001)\ncriterion = nn.CrossEntropyLoss(ignore_index=text_vocab[PADDING_TOKEN])\n\n# Train the model","metadata":{"_uuid":"d6237358-405f-451a-b9d9-feff20b80984","_cell_guid":"28fcf2e5-7c26-4daa-809e-3bbcc036d2fb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size = 256  # Adjust based on your GPU memory\ntrain_data = (train_morse, train_text)\nval_data = (val_morse, val_text)\nnum_epochs = 500\n\nprint(train_morse[1], train_text[1])\nprint(data[1])","metadata":{"_uuid":"aaebc2f8-a6ac-495c-8bf5-f08f9db34518","_cell_guid":"55303bff-2db0-4eae-9361-3633900376f3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n\n## Visualization and Model Inspection\n\nWe've included additional tools for model understanding:\n- `torchview` for computational graph visualization\n- `torchinfo` for detailed model architecture summary\n- Matplotlib plots for training and validation metrics","metadata":{"_uuid":"71b97e3f-e728-4414-92bb-725a44e3f862","_cell_guid":"787a8511-d0c7-497e-a6a6-579eb935961f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"\ngraphviz.set_jupyter_format('png')\n\nmodel_graph = draw_graph(model, input_size=[(2, MAX_MORSE_LEN), (2, MAX_TEXT_LEN - 1)], expand_nested=True, device=device, dtypes=[torch.long, torch.long])\nmodel_graph.visual_graph","metadata":{"_uuid":"391c9ad2-9eb9-4cf9-b793-ecc001d378ed","_cell_guid":"0cdfe1e2-3848-41ed-bd63-228aa01fcac2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"summary(model, input_size=[(2, MAX_MORSE_LEN), (2, MAX_TEXT_LEN - 1)] ,device=device, dtypes=[torch.long, torch.long])","metadata":{"_uuid":"536301e8-a296-4fb9-a40e-514f189e10d9","_cell_guid":"f20ca50c-5a67-4606-a7a5-509baca5ec55","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(sum(p.numel() for p in model.parameters()))","metadata":{"_uuid":"bc96b451-a6f4-4586-b5e1-a219a8acc137","_cell_guid":"ad59f418-f42a-4211-9264-cd35a35f63df","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = train(model, train_data, val_data, optimizer, criterion, num_epochs, batch_size)","metadata":{"_uuid":"7e767dd6-d501-47ac-bd3e-c0c0e5051488","_cell_guid":"131cc46e-6e1f-4586-90dc-e80ae2ce486b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nplt.plot(history[\"Loss_Train\"])\nplt.plot(history[\"Loss_Valid\"])\nplt.show()","metadata":{"_uuid":"9083f4f0-f463-496a-8458-ea369cea6c21","_cell_guid":"430ae620-c5c6-4540-8a05-d5a770b23429","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(history[\"Accuracy_Train\"])\nplt.plot(history[\"Accuracy_Valid\"])\nplt.show()","metadata":{"_uuid":"9648ba35-8835-4d62-a220-fd90dbdf67c6","_cell_guid":"57e5c800-436f-47cb-bb97-8c3d34256fc4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluation\nval_loss, val_acc = evaluate(model, val_data, criterion)\nprint(f\"Final Validation Loss: {val_loss:.4f}, Final Validation Accuracy: {val_acc:.4f}\")","metadata":{"_uuid":"5755079b-85ce-4987-8bfc-38dc1b0e1174","_cell_guid":"7cdacb6b-f101-458d-9c39-28fa2a1e5a18","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Inference and Translation\n\nThe `translate_sequence()` function demonstrates **how to use our trained model** for actual Morse code to text translation:\n\n### Translation Process:\n1. Preprocess input Morse sequence\n2. Generate text tokens sequentially\n3. Stop at End-of-Sequence (EOS) token\n4. Convert token indices back to characters","metadata":{"_uuid":"a3572575-e630-452d-be28-b7c2a043fb4d","_cell_guid":"0503161f-6e6f-48a5-9441-607d46d4b22e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def translate_sequence(binary_sequence, model, morse_vocab, text_vocab, reverse_text_vocab, max_morse_len):\n    \"\"\"\n    Translates a binary Morse sequence (with spaces) into text using the trained model.\n    \n    Args:\n        binary_sequence (str): Input Morse sequence in binary (e.g., \"01 10 111\").\n        model (nn.Module): Trained Transformer model.\n        morse_vocab (dict): Vocabulary mapping for Morse code.\n        text_vocab (dict): Vocabulary mapping for text.\n        reverse_text_vocab (dict): Reverse mapping from text indices to characters.\n        max_morse_len (int): Maximum length of Morse sequences (for padding).\n    \n    Returns:\n        str: Translated text sequence.\n    \"\"\"\n    # Preprocess the input binary sequence\n    \n    morse_tensor = [morse_vocab[SOS_TOKEN]] + [morse_vocab[ch] for ch in binary_sequence if ch in morse_vocab] + [morse_vocab[EOS_TOKEN]]\n    morse_tensor += [morse_vocab[PADDING_TOKEN]] * (max_morse_len - len(morse_tensor))\n    morse_tensor = torch.tensor(morse_tensor, dtype=torch.long).unsqueeze(0).to(device)\n    \n    # Prepare the target input (start with <SOS>)\n    tgt_input = torch.tensor([text_vocab[SOS_TOKEN]], dtype=torch.long).unsqueeze(0).to(device)\n\n    model.eval()\n    with torch.no_grad():\n        for _ in range(MAX_TEXT_LEN):  # Generate until max text length\n            output = model(morse_tensor, tgt_input)\n            next_token = output.argmax(2)[:, -1:]  # Get the next token\n            tgt_input = torch.cat((tgt_input, next_token), dim=1)  # Append the next token\n\n            if next_token.item() == text_vocab[EOS_TOKEN]:  # Stop if <EOS> is generated\n                break\n\n    # Convert output indices to text\n    translated_text = ''.join(reverse_text_vocab[idx.item()] for idx in tgt_input[0, 1:-1])  # Exclude <SOS> and <EOS>\n    return translated_text\n\n# Example usage\nbinary_sequence = \"00 10 1000 000 11 01 101\"  # Example Morse input (binary with spaces)\ntranslated_text = translate_sequence(binary_sequence, model, morse_vocab, text_vocab, reverse_text_vocab, MAX_MORSE_LEN)\nprint(f\"Input Morse Sequence: {binary_sequence}\")\nprint(f\"Translated Text: {translated_text}\")","metadata":{"_uuid":"a77e940c-faef-4e33-9889-70fdaf93bd97","_cell_guid":"efd327b1-8fae-40fb-a91f-ae934bdcd91e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Learning Outcome\nBy the end of this tutorial, you'll have:\n- Understood Transformer architecture fundamentals\n- Implemented a sequence-to-sequence translation model\n- Learned practical PyTorch implementation techniques\n- Explored an interesting domain-specific translation task","metadata":{"_uuid":"f1a4b396-7db1-4b41-b5ed-59153fcd8729","_cell_guid":"5dde218e-0fe4-4245-a12c-60357b3a2160","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## References\n- [\"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762) - Original Transformer paper\n- [Detailed PyTorch Transformer Guide](https://towardsdatascience.com/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1) - The current implementation is inspired by this work. \n\n","metadata":{"_uuid":"1595c50f-c268-4cc2-9a35-d745ae70c1e9","_cell_guid":"ea62a7a3-93de-4c51-8b86-c85a1650eaab","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}}]}